{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from pprint import pprint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['from', 'city', 'tall', 'building', 'image', 'capture', 'street', 'white', 'car', 'black', 'sky', 'table', 'moped', 'red', 'two', 'house', 'water', 'boat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = pd.read_csv('results/post_survey.csv')\n",
    "caption_a = main_data['caption_A'].tolist()\n",
    "caption_b = main_data['caption_B'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "caption_a = [simple_preprocess(str(doc), deacc=True) for doc in caption_a]\n",
    "caption_b = [simple_preprocess(str(doc), deacc=True) for doc in caption_b]\n",
    "\n",
    "# Remove stopwords\n",
    "caption_a = [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in caption_a]\n",
    "caption_b = [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in caption_b]\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "caption_a = [[lemmatizer.lemmatize(word) for word in simple_preprocess(str(doc))] for doc in caption_a]\n",
    "caption_b = [[lemmatizer.lemmatize(word) for word in simple_preprocess(str(doc))] for doc in caption_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 most common words in the captions - caption_a and caption_b\n",
    "def get_top_words(caption):\n",
    "    words = [word for doc in caption for word in doc]\n",
    "    word_freq = Counter(words)\n",
    "    top_words = word_freq.most_common(10)\n",
    "    return top_words\n",
    "\n",
    "top_words_a = get_top_words(caption_a)\n",
    "top_words_b = get_top_words(caption_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in caption A: [('capture', 32), ('scene', 29), ('heart', 25), ('serene', 24), ('bustling', 23), ('vibrant', 17), ('stand', 14), ('urban', 12), ('adorned', 11), ('glow', 11)]\n",
      "Top 10 words in caption B: [('capture', 54), ('scene', 48), ('serene', 27), ('vibrant', 21), ('bustling', 20), ('dominating', 19), ('heart', 18), ('large', 16), ('color', 13), ('stand', 12)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 words in caption A:', top_words_a)\n",
    "print('Top 10 words in caption B:', top_words_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cozy', 'kitchen', 'scene', 'refrigerator', 'standing', 'amidst', 'backdrop', 'wooden', 'cabinet', 'shelf', 'adorned', 'various', 'item', 'touch', 'personal', 'flair']]\n",
      "[['capture', 'cozy', 'kitchen', 'scene', 'dominating', 'center', 'frame', 'large', 'refrigerator', 'standing', 'sleek', 'right', 'wooden', 'cabinet', 'glass', 'door', 'showcase', 'array', 'dish', 'glassware']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "caption_a = list(sent_to_words(caption_a))\n",
    "caption_b = list(sent_to_words(caption_b))\n",
    "\n",
    "print(caption_a[:1])\n",
    "print(caption_b[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "caption_a_id2word = corpora.Dictionary(caption_a)\n",
    "caption_b_id2word = corpora.Dictionary(caption_b)\n",
    "\n",
    "# Create Corpus\n",
    "caption_a_texts = caption_a\n",
    "caption_b_texts = caption_b\n",
    "\n",
    "# Term Document Frequency\n",
    "caption_a_corpus = [caption_a_id2word.doc2bow(text) for text in caption_a_texts]\n",
    "caption_b_corpus = [caption_b_id2word.doc2bow(text) for text in caption_b_texts]\n",
    "\n",
    "# View\n",
    "print(caption_a_corpus[:1])\n",
    "print(caption_b_corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "caption_a_lda_model = LdaModel(corpus=caption_a_corpus,\n",
    "                               id2word=caption_a_id2word,\n",
    "                               num_topics=5,\n",
    "                               random_state=100,\n",
    "                               update_every=1,\n",
    "                               chunksize=100,\n",
    "                               passes=10,\n",
    "                               alpha='auto',\n",
    "                               per_word_topics=True)\n",
    "\n",
    "caption_b_lda_model = LdaModel(corpus=caption_b_corpus,\n",
    "                                id2word=caption_b_id2word,\n",
    "                                num_topics=5,\n",
    "                                random_state=100,\n",
    "                                update_every=1,\n",
    "                                chunksize=100,\n",
    "                                passes=10,\n",
    "                                alpha='auto',\n",
    "                                per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.013*\"stand\" + 0.013*\"heart\" + 0.011*\"serene\" + 0.010*\"cozy\" + '\n",
      "  '0.010*\"coffee\" + 0.010*\"bench\" + 0.007*\"capture\" + 0.007*\"woman\" + '\n",
      "  '0.007*\"bustling\" + 0.007*\"blue\"'),\n",
      " (1,\n",
      "  '0.019*\"scene\" + 0.019*\"capture\" + 0.018*\"bustling\" + 0.016*\"vibrant\" + '\n",
      "  '0.016*\"heart\" + 0.010*\"serene\" + 0.010*\"urban\" + 0.009*\"color\" + '\n",
      "  '0.008*\"boat\" + 0.008*\"stand\"'),\n",
      " (2,\n",
      "  '0.016*\"bustling\" + 0.016*\"heart\" + 0.013*\"monitor\" + 0.013*\"woman\" + '\n",
      "  '0.010*\"desk\" + 0.010*\"scene\" + 0.010*\"computer\" + 0.007*\"amidst\" + '\n",
      "  '0.007*\"home\" + 0.007*\"sidewalk\"'),\n",
      " (3,\n",
      "  '0.018*\"cozy\" + 0.011*\"capture\" + 0.011*\"heart\" + 0.011*\"set\" + 0.011*\"glow\" '\n",
      "  '+ 0.011*\"bathed\" + 0.011*\"scene\" + 0.011*\"kitchen\" + 0.011*\"wooden\" + '\n",
      "  '0.008*\"vibrant\"'),\n",
      " (4,\n",
      "  '0.024*\"capture\" + 0.024*\"serene\" + 0.016*\"scene\" + 0.014*\"nestled\" + '\n",
      "  '0.010*\"glow\" + 0.010*\"lush\" + 0.008*\"row\" + 0.008*\"soft\" + 0.008*\"green\" + '\n",
      "  '0.008*\"roof\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(caption_a_lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.026*\"capture\" + 0.023*\"scene\" + 0.019*\"serene\" + 0.014*\"boat\" + '\n",
      "  '0.013*\"painted\" + 0.012*\"backdrop\" + 0.011*\"stand\" + 0.011*\"standing\" + '\n",
      "  '0.011*\"shade\" + 0.011*\"amidst\"'),\n",
      " (1,\n",
      "  '0.028*\"capture\" + 0.026*\"scene\" + 0.016*\"vibrant\" + 0.013*\"bustling\" + '\n",
      "  '0.010*\"dominating\" + 0.009*\"tree\" + 0.009*\"large\" + 0.008*\"standing\" + '\n",
      "  '0.008*\"heart\" + 0.008*\"man\"'),\n",
      " (2,\n",
      "  '0.025*\"heart\" + 0.014*\"seated\" + 0.014*\"woman\" + 0.014*\"bustling\" + '\n",
      "  '0.009*\"vibrant\" + 0.009*\"cozy\" + 0.009*\"man\" + 0.009*\"attention\" + '\n",
      "  '0.009*\"kitchen\" + 0.009*\"truck\"'),\n",
      " (3,\n",
      "  '0.036*\"capture\" + 0.030*\"scene\" + 0.022*\"serene\" + 0.016*\"color\" + '\n",
      "  '0.014*\"dominating\" + 0.013*\"facade\" + 0.013*\"architecture\" + 0.013*\"modern\" '\n",
      "  '+ 0.011*\"large\" + 0.011*\"reflecting\"'),\n",
      " (4,\n",
      "  '0.030*\"capture\" + 0.025*\"scene\" + 0.015*\"serene\" + 0.014*\"dominating\" + '\n",
      "  '0.013*\"wall\" + 0.012*\"green\" + 0.012*\"soft\" + 0.012*\"light\" + 0.010*\"stone\" '\n",
      "  '+ 0.010*\"vibrant\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(caption_b_lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in caption A LDA: [[('stand', 0.013256938)], [('scene', 0.019245142)], [('bustling', 0.016315464)], [('cozy', 0.018314531)], [('capture', 0.023877779)]]\n",
      "Top 10 words in caption B LDA: [[('capture', 0.025909618)], [('capture', 0.0284813)], [('heart', 0.024514847)], [('capture', 0.035603095)], [('capture', 0.029718285)]]\n"
     ]
    }
   ],
   "source": [
    "# Print the top 10 words in each topic\n",
    "def get_top_words_lda(lda_model):\n",
    "    top_words = []\n",
    "    for i in range(5):\n",
    "        top_words.append(lda_model.show_topic(i, 1))\n",
    "    return top_words\n",
    "\n",
    "top_words_a_lda = get_top_words_lda(caption_a_lda_model)\n",
    "top_words_b_lda = get_top_words_lda(caption_b_lda_model)\n",
    "\n",
    "print('Top 10 words in caption A LDA:', top_words_a_lda)\n",
    "print('Top 10 words in caption B LDA:', top_words_b_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.593530161224401\n",
      "\n",
      "Perplexity:  -6.43324910353426\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', caption_a_lda_model.log_perplexity(caption_a_corpus))  # a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity: ', caption_b_lda_model.log_perplexity(caption_b_corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score for caption A:  0.32544308491555546\n",
      "Coherence Score for caption B:  0.29999396279139234\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "caption_a_coherence_model_lda = CoherenceModel(model=caption_a_lda_model, texts=caption_a, dictionary=caption_a_id2word, coherence='c_v')\n",
    "caption_a_coherence_lda = caption_a_coherence_model_lda.get_coherence()\n",
    "print('Coherence Score for caption A: ', caption_a_coherence_lda)\n",
    "\n",
    "caption_b_coherence_model_lda = CoherenceModel(model=caption_b_lda_model, texts=caption_b, dictionary=caption_b_id2word, coherence='c_v')\n",
    "caption_b_coherence_lda = caption_b_coherence_model_lda.get_coherence()\n",
    "print('Coherence Score for caption B: ', caption_b_coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis.gensim\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('results/ldavis_prepared_'+str(5))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(caption_a_lda_model, caption_a_corpus, caption_a_id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "pyLDAvis.save_html(LDAvis_prepared, 'results/ldavis_prepared_'+ str(5) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (2.2.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (3.1.3)\n",
      "Requirement already satisfied: numexpr in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (2.8.7)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: gensim in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pyLDAvis) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from gensim->pyLDAvis) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: wrapt in /Users/anugrahvaishnav/anaconda3/envs/transformers/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n",
      "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m942.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: funcy, pyLDAvis\n",
      "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyLDAvis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
