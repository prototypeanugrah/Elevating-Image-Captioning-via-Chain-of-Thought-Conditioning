{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_rachel_melamed_uml_edu/anugrah_vaishnav_student_uml_edu-conda/envs/anuenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import clip\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import argparse\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A romantic dinner for two, with a beautiful blonde woman engrossed in her tablet, while her companion looks on, surrounded by a table set for a delightful meal.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "string = 'Generate a caption for this image in 50 words[/INST] \"A romantic dinner for two, with a beautiful blonde woman engrossed in her tablet, while her companion looks on, surrounded by a table set for a delightful meal.\"'\n",
    "# string.split(\"[/INST]\")[-1].strip().split('\"')[1]\n",
    "re.findall(r'\"(.*?)\"', string)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_dir, survey_dir, num_sample_images, user_name):\n",
    "\n",
    "    if not os.path.exists(survey_dir):\n",
    "        os.makedirs(survey_dir)\n",
    "\n",
    "    # if os.path.exists(os.path.join(survey_dir, f'{user_name}_survey.csv')):\n",
    "    #     print(f\"Survey file for {user_name} already exists. Please delete the file and try again.\")\n",
    "    #     return None\n",
    "\n",
    "    # Get the images from the input file\n",
    "    input_df = pd.read_csv(input_dir)\n",
    "\n",
    "    # Use images that do not have any captions\n",
    "    input_df = input_df[input_df[\"caption_A\"].isnull() & input_df[\"caption_B\"].isnull()]\n",
    "\n",
    "    # Sample the images\n",
    "    input_df = input_df.sample(num_sample_images).reset_index(drop=True)\n",
    "    results_df = input_df.copy()\n",
    "\n",
    "    # Get the models and processors\n",
    "    llava_model, llava_processor = get_model_processor(\"llava\")\n",
    "    # clip_model, clip_processor = get_model_processor(\"clip\")\n",
    "\n",
    "    for image_url in input_df.image_url.values:\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "        # Generate Captions:\n",
    "        # WCOT: without chain-of-thought/without any conditioned prompt\n",
    "        # COT: with chain-of-thought/with a pre-determined conditioned prompt\n",
    "        wcot_caption, cot_caption = generate_caption_from_image(\n",
    "            llava_model, llava_processor, image\n",
    "        )\n",
    "        print(f\"WCOT Caption: {wcot_caption}\")\n",
    "        print(f\"COT Caption: {cot_caption}\")\n",
    "\n",
    "        # Process the generated captions\n",
    "        process_wcot_caption = process_caption(wcot_caption)\n",
    "        process_cot_caption = process_caption(cot_caption)\n",
    "        print(f\"Processed WCOT Caption: {process_wcot_caption}\")\n",
    "        print(f\"Processed COT Caption: {process_cot_caption}\")\n",
    "\n",
    "        valid_outcomes = [process_wcot_caption, cot_caption]\n",
    "\n",
    "        # Evaluate which caption is better according to LLM\n",
    "        # clip_pred = get_predicted_labels_clip(\n",
    "        #     clip_model, clip_processor, image, valid_outcomes\n",
    "        # )\n",
    "\n",
    "        # Evaluate which caption is better according to LLM\n",
    "        llm_pred = get_predicted_labels_llm(\n",
    "            llava_model, llava_processor, image, valid_outcomes, max_new_tokens=50\n",
    "        )\n",
    "\n",
    "        # Evaluate which caption is better according to a human\n",
    "        # human_pred = get_human_pref_caption(image, valid_outcomes)\n",
    "\n",
    "        results_df.loc[-1] = [\n",
    "            image_url,\n",
    "            process_wcot_caption,\n",
    "            process_cot_caption,\n",
    "            \" \",\n",
    "            llm_pred,\n",
    "            \" \",\n",
    "        ]\n",
    "        results_df.index = results_df.index + 1\n",
    "        results_df = results_df.sort_index()\n",
    "\n",
    "    # Save the results\n",
    "    results_df.to_csv(\n",
    "        os.path.join(user_survey_dir, f\"{user_name}_survey.csv\")\n",
    "    )\n",
    "\n",
    "\n",
    "def get_images(input_file, output_file, num_images=10):\n",
    "    objects = pd.read_csv(input_file)[\"image_url\"]\n",
    "\n",
    "    random_numbers = random.sample(range(0, len(objects)), num_images)\n",
    "    images = [objects[i] for i in random_numbers]\n",
    "\n",
    "    output_file = output_file.append(pd.DataFram([images], columns=[\"image_url\"]))\n",
    "\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def get_model_processor(model_name):\n",
    "    if model_name == \"llava\":\n",
    "        model_pref = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "        processor = LlavaNextProcessor.from_pretrained(model_pref)\n",
    "\n",
    "        model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            model_pref, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.to(device)\n",
    "    elif model_name == \"clip\":\n",
    "        model, processor = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    print(f\"Model: {model_name} loaded successfully!\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def generate_caption_from_image(model, processor, image, max_new_tokens=50):\n",
    "\n",
    "    # Generate prompt\n",
    "    wcot_prompt = \"[INST] <image>\\nGenerate a caption for this image in 50 words[/INST]\"\n",
    "    cot_prompt = \"[INST] <image>\\nGenerate a caption for this image, and the description should include the number of objects in the image without explicitly mentioning it in 50 words[/INST]\"\n",
    "\n",
    "    # Process prompt\n",
    "    wcot_inputs = processor(wcot_prompt, image, return_tensors=\"pt\").to(device)\n",
    "    cot_inputs = processor(cot_prompt, image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # autoregressively complete prompt\n",
    "    wcot_output = model.generate(**wcot_inputs, max_new_tokens=max_new_tokens)\n",
    "    cot_output = model.generate(**cot_inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Decode output\n",
    "    wcot_final_output = processor.decode(wcot_output[0], skip_special_tokens=True)\n",
    "    cot_final_output = processor.decode(cot_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(wcot_final_output)\n",
    "\n",
    "    # Process the output\n",
    "    wcot_final_output = (\n",
    "        wcot_final_output.split(\"[/INST]\")[-1].strip().split('\"')[1]\n",
    "    )  # remove prompt\n",
    "    cot_final_output = (\n",
    "        cot_final_output.split(\"[/INST]\")[-1].strip().split('\"')[1]\n",
    "    )  # remove prompt\n",
    "\n",
    "    return wcot_final_output, cot_final_output\n",
    "\n",
    "\n",
    "def process_caption(caption):\n",
    "    # first_split = caption.split(\"[/INST]\")[-1].strip() # remove prompt\n",
    "    second_split = caption.split(\". \")  # split into sentences\n",
    "\n",
    "    # remove last sentence if it doesn't end with a period\n",
    "    if not second_split[-1].endswith(\".\"):\n",
    "        second_split.pop()\n",
    "\n",
    "    final_string = \". \".join(second_split)  # join sentences\n",
    "    return final_string\n",
    "\n",
    "\n",
    "def get_predicted_labels_clip(model, preprocess, image, captions):\n",
    "\n",
    "    img = preprocess(image).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize(captions).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_per_image, _ = model(img, text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    final_pred = probs[0].argmax()\n",
    "    print(\"Label probs:\", final_pred)\n",
    "\n",
    "    # if final_pred == 0:\n",
    "    #     print(\"WCOT\", wcot_caption)\n",
    "    # else:\n",
    "    #     print(\"COT\", cot_caption)\n",
    "\n",
    "    if final_pred == 0:\n",
    "        return \"A\"\n",
    "    else:\n",
    "        return \"B\"\n",
    "\n",
    "\n",
    "def get_predicted_labels_llm(model, processor, image, captions, max_new_tokens=50):\n",
    "\n",
    "    # Generate prompt\n",
    "    prompt = f\"[INST] <image>\\nWhich caption is a better choice for the given image: (A) {captions[0]} or (B) {captions[1]}? Give only the option letter to me.[/INST]\"\n",
    "\n",
    "    # Process prompt\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # autoregressively complete prompt\n",
    "    output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Decode output\n",
    "    final_output = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    final_output = final_output.split(\"[/INST]\")[-1].strip()  # remove prompt\n",
    "\n",
    "    if final_output == \"A\":\n",
    "        # print(f\"Selected caption: WCOT: {captions[0]}\")\n",
    "        return 0\n",
    "    else:\n",
    "        # print(f\"Selected caption: COT: {captions[1]}\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_human_pref_caption(image, captions):\n",
    "    image.show()\n",
    "    print(\"There are 2 captions. Which one do you prefer from the ones?\")\n",
    "    \n",
    "    # Shuffle the captions to avoid bias\n",
    "    random.shuffle(captions)\n",
    "    \n",
    "    print(f\"Caption (A): {captions[0]}\")\n",
    "    print(f\"Caption (B): {captions[1]}\")\n",
    "\n",
    "    possible_inputs = [\"A\", \"B\"]\n",
    "    for attempts in range(3):\n",
    "        human_input = input(f\"Attempt: {attempts+1}. Enter either option 'A' or 'B'\")\n",
    "        if human_input in possible_inputs:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please try again!\")\n",
    "\n",
    "    return human_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:06<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llava loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m num_sample_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      4\u001b[0m user_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_user\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurvey_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sample_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(input_dir, survey_dir, num_sample_images, user_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(image_url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Generate Captions:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# WCOT: without chain-of-thought/without any conditioned prompt\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# COT: with chain-of-thought/with a pre-determined conditioned prompt\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m wcot_caption, cot_caption \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_caption_from_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllava_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllava_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWCOT Caption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwcot_caption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOT Caption: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcot_caption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 124\u001b[0m, in \u001b[0;36mgenerate_caption_from_image\u001b[0;34m(model, processor, image, max_new_tokens)\u001b[0m\n\u001b[1;32m    120\u001b[0m cot_final_output \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(cot_output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Process the output\u001b[39;00m\n\u001b[1;32m    123\u001b[0m wcot_final_output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mwcot_final_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[/INST]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    125\u001b[0m )  \u001b[38;5;66;03m# remove prompt\u001b[39;00m\n\u001b[1;32m    126\u001b[0m cot_final_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    127\u001b[0m     cot_final_output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    128\u001b[0m )  \u001b[38;5;66;03m# remove prompt\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wcot_final_output, cot_final_output\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "input_dir = 'images/visual_genome_available_images.csv'\n",
    "survey_dir = 'surveys'\n",
    "num_sample_images = 2\n",
    "user_name = 'test_user'\n",
    "\n",
    "main(input_dir, survey_dir, num_sample_images, user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python image_caption_prompt_analysis.py --input_dir images/visual_genome_available_images.csv --survey_dir surveys --num_sample_images 2 --user_name test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input_dir INPUT_DIR] --survey_dir\n",
      "                             SURVEY_DIR\n",
      "                             [--num_sample_images NUM_SAMPLE_IMAGES]\n",
      "                             --user_name USER_NAME\n",
      "ipykernel_launcher.py: error: the following arguments are required: --survey_dir, --user_name\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Generate captions for the given image and evaluate the captions using LLM and CLIP models.\"\n",
    ")\n",
    "# parser.add_argument(\"--image_url\", type=str, default='https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg', help=\"URL of the image\")\n",
    "parser.add_argument(\n",
    "    \"--input_dir\",\n",
    "    type=str,\n",
    "    required=False,\n",
    "    help=\"File containing the URLs of the images to be evaluated\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--survey_dir\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"CSV file containing all the survey results conducted\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sample_images\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"Number of images to be sampled for the survey\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--user_name\", type=str, required=True, help=\"Name of the survey taker.\"\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('results/generated_captions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_url</th>\n",
       "      <th>caption_A</th>\n",
       "      <th>caption_B</th>\n",
       "      <th>human_output</th>\n",
       "      <th>llm_output</th>\n",
       "      <th>clip_small_output</th>\n",
       "      <th>clip_large_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>In the tranquil expanse of a grassy field, a g...</td>\n",
       "      <td>In the tranquil expanse of a grassy field, a g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>A cozy kitchen scene with a white refrigerator...</td>\n",
       "      <td>The image captures a cozy kitchen scene. Domin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>The image captures a well-organized workspace,...</td>\n",
       "      <td>The image captures a well-organized workspace....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>The image captures a serene urban scene, where...</td>\n",
       "      <td>The image captures a serene urban scene. A bla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>Elegant Living Room: A Symphony of Comfort and...</td>\n",
       "      <td>The image captures a warm and inviting living ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          image_url  \\\n",
       "0           0  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "1           1  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "2           2  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "3           3  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "4           4  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "\n",
       "                                           caption_A  \\\n",
       "0  In the tranquil expanse of a grassy field, a g...   \n",
       "1  A cozy kitchen scene with a white refrigerator...   \n",
       "2  The image captures a well-organized workspace,...   \n",
       "3  The image captures a serene urban scene, where...   \n",
       "4  Elegant Living Room: A Symphony of Comfort and...   \n",
       "\n",
       "                                           caption_B  human_output  \\\n",
       "0  In the tranquil expanse of a grassy field, a g...           NaN   \n",
       "1  The image captures a cozy kitchen scene. Domin...           NaN   \n",
       "2  The image captures a well-organized workspace....           NaN   \n",
       "3  The image captures a serene urban scene. A bla...           NaN   \n",
       "4  The image captures a warm and inviting living ...           NaN   \n",
       "\n",
       "   llm_output  clip_small_output  clip_large_output  \n",
       "0         0.0                0.0                1.0  \n",
       "1         0.0                1.0                1.0  \n",
       "2         0.0                1.0                1.0  \n",
       "3         0.0                1.0                1.0  \n",
       "4         1.0                1.0                0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(file.shape)\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the last 3 columns to integers\n",
    "file['clip_small_output'] = file['clip_small_output'].apply(lambda x: int(x))\n",
    "file['clip_large_output'] = file['clip_large_output'].apply(lambda x: int(x))\n",
    "file['llm_output'] = file['llm_output'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>caption_A</th>\n",
       "      <th>caption_B</th>\n",
       "      <th>human_output</th>\n",
       "      <th>llm_output</th>\n",
       "      <th>clip_small_output</th>\n",
       "      <th>clip_large_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>In the tranquil expanse of a grassy field, a g...</td>\n",
       "      <td>In the tranquil expanse of a grassy field, a g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>A cozy kitchen scene with a white refrigerator...</td>\n",
       "      <td>The image captures a cozy kitchen scene. Domin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>The image captures a well-organized workspace,...</td>\n",
       "      <td>The image captures a well-organized workspace....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>The image captures a serene urban scene, where...</td>\n",
       "      <td>The image captures a serene urban scene. A bla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://cs.stanford.edu/people/rak248/VG_100K_...</td>\n",
       "      <td>Elegant Living Room: A Symphony of Comfort and...</td>\n",
       "      <td>The image captures a warm and inviting living ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  \\\n",
       "0  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "1  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "2  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "3  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "4  https://cs.stanford.edu/people/rak248/VG_100K_...   \n",
       "\n",
       "                                           caption_A  \\\n",
       "0  In the tranquil expanse of a grassy field, a g...   \n",
       "1  A cozy kitchen scene with a white refrigerator...   \n",
       "2  The image captures a well-organized workspace,...   \n",
       "3  The image captures a serene urban scene, where...   \n",
       "4  Elegant Living Room: A Symphony of Comfort and...   \n",
       "\n",
       "                                           caption_B  human_output  \\\n",
       "0  In the tranquil expanse of a grassy field, a g...           NaN   \n",
       "1  The image captures a cozy kitchen scene. Domin...           NaN   \n",
       "2  The image captures a well-organized workspace....           NaN   \n",
       "3  The image captures a serene urban scene. A bla...           NaN   \n",
       "4  The image captures a warm and inviting living ...           NaN   \n",
       "\n",
       "   llm_output  clip_small_output  clip_large_output  \n",
       "0           0                  0                  1  \n",
       "1           0                  1                  1  \n",
       "2           0                  1                  1  \n",
       "3           0                  1                  1  \n",
       "4           1                  1                  0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Small Accuracy: 0.574\n",
      "CLIP Large Accuracy: 0.518\n"
     ]
    }
   ],
   "source": [
    "# treat the llm_output as the ground truth and calculate the accuracy of the clip_small_output and clip_large_output\n",
    "file['clip_small_correct'] = file['clip_small_output'] == file['llm_output']\n",
    "file['clip_large_correct'] = file['clip_large_output'] == file['llm_output']\n",
    "\n",
    "clip_small_accuracy = file['clip_small_correct'].sum() / file.shape[0]\n",
    "clip_large_accuracy = file['clip_large_correct'].sum() / file.shape[0]\n",
    "\n",
    "print(f\"CLIP Small Accuracy: {clip_small_accuracy}\")\n",
    "print(f\"CLIP Large Accuracy: {clip_large_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
